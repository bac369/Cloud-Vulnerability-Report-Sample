# Cloud Vulnerability Assessment
## Part 1
### Testing to to meet Control Standards outlined in the CIS Benchmarks Handbook. 
# Kubernetes
### Control 1: 5.2.1
##### Code: [django-psp.yaml](/GiftcardSite/k8/django-psp.yaml)
1. Running `kubectl get psp <name> -o=jsonpath='{.spec.privileged}'` returns true and the PSP `privileged = true`.
![Pic2](/Report/artifacts/bac369-521a.png)
2. I set `privileged: false` and rebuilt the image.
3. Running `kubectl get psp <name> -o=jsonpath='{.spec.privileged}'` no longer returns true. This passes the Control as outlined in Benchmarks Documentation. Privileged containers are dangerous and can do most of the things the host can do. This change makes the contaienr no longer privileged.
![Pic2](/Report/artifacts/bac369-521b.png)

### Control 2: 5.2.2
##### Code: [django-psp.yaml](/GiftcardSite/k8/django-psp.yaml)
1. Running   `kubectl get psp <name> -o=jsonpath='{.spec.hostPID}'` returns true and the PSP `hostPID: true`.
![Pic2](/Report/artifacts/bac369-522a.png)
2. I set `hostPID: false` and rebuilt the image.
3. Running `kubectl get psp <name> -o=jsonpath='{.spec.hostPID}'` no longer returns true. This passes the Control as outlined in Benchmarks Documentation. This prevents the container from using the host process id namespace from inspecting processes outside the container.
![Pic2](/Report/artifacts/bac369-522b.png)

### Control 3: 5.2.3
##### Code: [django-psp.yaml](/GiftcardSite/k8/django-psp.yaml)
1. Running  `kubectl get psp <name> -o=jsonpath='{.spec.hostIPC}'` returns true and the PSP `hostIPC: true`.
![Pic2](/Report/artifacts/bac369-523a.png)
2. I set `hostIPC: false` and rebuilt the image.
3. Running `kubectl get psp <name> -o=jsonpath='{.spec.hostIPC}'` no longer returns true. This passes the Control as outlined in Benchmarks Documentation. This prevents the container from using the hosts IPC namespace to access processes outside the container.
![Pic2](/Report/artifacts/bac369-523b.png)

### Control 4: 5.2.4
##### Code: [django-psp.yaml](/GiftcardSite/k8/django-psp.yaml)
1. Running  `kubectl get psp <name> -o=jsonpath='{.spec.hostNetwork}'` returns true and the PSP `hostIPC: true`.
![Pic2](/Report/artifacts/bac369-524a.png)
2. I set `hostNetwork: false` and rebuilt the image.
3. Running `kubectl get psp <name> -o=jsonpath='{.spec.hostNetwork}'` no longer returns true. This passes the Control as outlined in Benchmarks Documentation. This prevents the container from accessing the host's namespace and access local loopback between pods.
![Pic2](/Report/artifacts/bac369-524b.png)

### Control 5: 5.2.5
##### Code: [django-psp.yaml](/GiftcardSite/k8/django-psp.yaml)
1. Running  `kubectl get psp <name> -o=jsonpath='{.spec.allowPrivilegeEscalation}'` returns true and the PSP `allowPrivilegeEscalation: true`.
![Pic2](/Report/artifacts/bac369-525a.png)
2. I set `hostNetwork: false` and rebuilt the image.
3. Running `kubectl get psp <name> -o=jsonpath='{.spec.allowPrivilegeEscalation}'` no longer returns true. This passes the Control as outlined in Benchmarks Documentation. This prevents exploitation that can be caused by child processes gaining more privleges than their parent.
![Pic2](/Report/artifacts/bac369-525b.png)

### Control 6: 5.2.6
##### Code: [django-psp.yaml](/GiftcardSite/k8/django-psp.yaml)
1. Running  `kubectl get psp <name> -o=jsonpath='{.spec.runAsUser.rule}'` returns `RunAsAny`.
![Pic2](/Report/artifacts/bac369-526a.png)
2. I set `runAsUser: rule: MustRunAsNonRoot` and rebuilt the image.
3. Running `kubectl get psp <name> -o=jsonpath='{.spec.runAsUser.rule}'` no longer returns true. This passes the Control as outlined in Benchmarks Documentation. This prevents container breakout that can be exploted by a root capable malicious actor.
![Pic2](/Report/artifacts/bac369-526b.png)

### Control 7: 5.2.7
##### Code: [django-psp.yaml](/GiftcardSite/k8/django-psp.yaml)
1. Running  `kubectl get psp <name> -o=jsonpath='{.spec.requiredDropCapabilities}'` returns nothing which fails the control.
![Pic2](/Report/artifacts/bac369-527a.png)
2. I added `requiredDropCapabilities: - ALL` and rebuilt the image. 
3. Running `kubectl get psp <name> -o=jsonpath='{.spec.requiredDropCapabilities}'` returns '["ALL"]'. This passes the Control as outlined in Benchmarks Documentation. This prevents containers from launching with the NET_RAW capability which can be exploited.
![Pic2](/Report/artifacts/bac369-527b.png)


### Control 8: 5.4.1
##### Code: [django-admin-pass-secret.yaml](/GiftcardSite/k8/django-admin-pass-secret.yaml), [django-keysecret.yaml](/GiftcardSite/k8/django-keysecret.yaml) , [django-deploy.yaml](/GiftcardSite/k8/django-deploy.yaml), [settings.py](/GiftcardSite/GiftCardSite/settings.py)
  1. I looked over `settings.py` and `views.py`. I found the `SECRET_KEY` variable was stored in the `settings.py` file.
![Pic2](/Report/artifacts/bac369-541a.png)
2. I created a new .yaml file using `django-admin-pass-secret.yaml` as a template called `django-keysecret.yaml` and changed settings.py to use this file. I also converted the key to base64.
![Pic2](/Report/artifacts/bac369-541c.png)
3. As you can see `settings.py` no longer stores the key in the code and uses the secrets .yaml file. The Web Application still works.
![Pic2](/Report/artifacts/bac369-541c.png)

### Control 9: 5.7.1
##### Code: [django-giftdeployment.yaml](/GiftcardSite/k8/django-giftdeployment.yaml) , Example namespace in metadata: [db-deployment.yaml](/db/k8/db-giftdeployment.yaml)
  1. As you can see below the only namespaces created were the defaults: default, kube-nodelease, kube-public, kube-system. All the pods were running within the default namespace, which is a bad practice.
![Pic2](/Report/artifacts/bac369-571a.png)
2. I created a new .yaml file using `django-giftdeployment.yaml` this file creates a new namespace called `giftdeployment`. Within the all the yaml files in the db, proxy, and GiftCardSite k8 folders, I added a line in the meta data section `namespace: giftdeployment`. When running `kubectl apply -f <file>` this automatically adds the pod to he giftdeployment namespace.
3. ![Pic2](/Report/artifacts/bac369-571c.png)
3. As you can see, pods all run within the new `giftdeployment` namespace. They do not cause errors, and when I run `minikube service proxy-service --namespace=giftdeployment` the Web application runs properly.
![Pic2](/Report/artifacts/bac369-571d.png)

### Control 10: 5.7.2
##### Code: Example securitycontext in spec: [django-deploy.yaml](/GiftcardSite/k8/django-deploy.yaml)
  1. As per the benchmark documentation, I looked through the deployment containers and saw there was no security profiles defined for the clusters. These security measuess prevent the usage of syscall within the cluster.
![Pic2](/Report/artifacts/bac369-572a.png)
  2. The docker/default profile is `RuntimeDefault`. I added the ` securityContext: seccompProfile: type: RuntimeDefault`. To each of the container configurations in the deployment files.
  ![Pic2](/Report/artifacts/bac369-572b.png)
3. As you can see, pods all run properly after applying the configuration changes. As per the audit guide, the lines defining the profile are present. They do not cause errors, and the Web application runs properly.
![Pic2](/Report/artifacts/bac369-572b.png)

## Docker
### Control 11: 4.1
##### Code: [GiftCardSite Dockerfile](/Dockerfile) , [DB Dockerfile](/db/Dockerfile) , [Proxy Dockerfile](/proxy/Dockerfile)
1. Run the command `docker ps --quiet | xargs --max-args=1 -I{} docker exec {} cat /proc/1/status| grep '^Uid:' | awk '{print $3}'`. Results in the screenshot below show that there are 3 containers running as root, where the UID is 0. According to the security review, two, the proxy and GiftCardSite, files need to be changed. Compromised root containers are more likely to breakout and escalate privilege. Containers should run as a less privileged User.
![Pic2](/Report/artifacts/bac369-41d.png)
2. The GiftCardSite already had the code for creating a user, so I uncommented out the commands. For proxy, I added `USER nginx` to switch the user to nginx once commands were finished.
![Pic2](/Report/artifacts/bac369-41c.png)
3. As you can see, pods all run properly after applying the configuration changes. As per the audit guide, the audit command returns two non-zero UID as a result of the changes to the two Dockerfiles. One UID is still 0 due to the MYSQL Dockerfiles still using root, but that remediation was not recommended in the Security Review doumcnetation. The Web application runs properly. 
![Pic2](/Report/artifacts/bac369-41b.png)


### Control 12: 4.2
  1. I ran the command `docker images` to find the list of images.
![Pic2](/Report/artifacts/bac369-42d.png)
  2. Ran `docker history <image name>` to look into each used image.
  ![Pic2](/Report/artifacts/bac369-42c.png)
3. Looking into each, none of the listed images seem suspicious. I agree with the security review that this control passes.
![Pic2](/Report/artifacts/bac369-42b.png)

### Control 12: 4.3
##### Code: [GiftCardSite Dockerfile](/Dockerfile)
  1. I have located the packages by looking into the main Dockerfile.
![Pic2](/Report/artifacts/bac369-43a.png)
  2. After investigation it does not seem like there are any unnecessary packages installed. I individually removed and rebuilt each one. For some, the application wouldn't build after removal. For others, the application would build, but errors would be thrown meaning something might not be working in the application. An example can be seen below:
  ![Pic2](/Report/artifacts/bac369-43b.png)
3. I would advise against the recommendation in the security review, packages should not be removed at the moment.
![Pic2](/Report/artifacts/bac369-43c.png)

### Control 14: 4.9
##### Code: [GiftCardSite Dockerfile](/Dockerfile) , [DB Dockerfile](/db/Dockerfile) , [Proxy Dockerfile](/proxy/Dockerfile)
1. I looked through the Dockerfiles and saw ADD was used a number of times in the GiftCardSite and db Dockerfiles. `ADD` is a security risk and may allow malicious files to be directly accessed without scanning.
![Pic2](/Report/artifacts/bac369-49a.png)
2. I swapped every `ADD` to `COPY`, and rebuilt docker.
3. With the `ADD` instructions swapped to `COPY`. The pods all run properly after applying the configuration changes. As per the audit guide, no `ADD` instructions remain. They do not cause errors, and the Web application runs properly.
![Pic2](/Report/artifacts/bac369-49b.png)

### Control 15: 4.10
##### Code:  [DB Dockerfile](/db/Dockerfile) , [django-keysecret.yaml](/GiftcardSite/k8/django-keysecret.yaml) , [django-deploy.yaml](/GiftcardSite/k8/django-deploy.yaml)
1. The db Dockerfile contained a `ENV MYSQL_ROOT_PASSWORD` which violated the audit.Neither of the other Dockerfiles contained any secrets.
![Pic2](/Report/artifacts/bac369-410a.png)
2. Altered the `django-deploy.yaml` file to retrieve the value from `django-keysecret.yaml` file I created before. I added the `MYSQLPS` entry and converted the `thisisatestthing.` password to base64.
![Pic2](/Report/artifacts/bac369-410b.png)
3. I rebuilt the docker image and applied the changes with `kubectl apply`. I ran the `proxy-service` to open the Web application and it ran properly.
![Pic2](/Report/artifacts/bac369-410b.png)

## MYSQL

### Control 16: 1.2
##### Code:  [setup.sql](/db/setup.sql)
  1. I accessed the mysql container shell, accessed mysql on root and ran `SELECT user FROM user`. I saw there were two root users mysql.schema, mysql.session, and mysql.sys, but there was no standard `mysql` user.
![Pic2](/Report/artifacts/bac369-m12a.png)
  2. I created the mysql user adding `CREATE USER 'mysql'@'localhost' IDENTIFIED BY 'testing1';` to setup.sql. I then granted it permissions by adding `GRANT <list of permissions> on *.* to 'mysql'@'localhost' with GRANT OPTION;`.
![Pic2](/Report/artifacts/bac369-m12bc.png)
3. I then reran `SELECT user FROM user` and the mysql user was in the list. I was also able to login to mysql with the new user and perform operations. This `mysql` user should be used instead of `root`.
![Pic2](/Report/artifacts/bac369-m12c.png)

### Control 17: 2.3
I looked through the deployment yaml, dockerfile, and the pod, and all the evidence  suggests that the machine is not solely to MYSQL. I agree with the security review.
![Pic2](/Report/artifacts/bac369-m23a.png)

### Control 18: 2.7
##### Code:  [setup.sql](/db/setup.sql)
1. I ran `mysql> SELECT VARIABLE_NAME, VARIABLE_VALUEFROM performance_schema.global_variables where VARIABLE_NAME like'default_password_lifetime';`. I saw that the `default_password_lifetime` was set to `0` which means the password doesn't expire.
![Pic2](/Report/artifacts/bac369-m27a.png)
2. Using the guidelines in the CIS benchmark, added `SET PERSIST default_password_lifetime = 365;` to setup.sql. This updated the mysql file and changed the default to 365 days.
![Pic2](/Report/artifacts/bac369-m2729b.png)
3. I rebuilt the databse and ran the command from 1) and saw the `default_password_lifetime` was now set to 365. This issue now passes.
![Pic2](/Report/artifacts/bac369-m2729b.png)

### Control 19: 2.9
##### Code:  [setup.sql](/db/setup.sql)
1. I ran `mysql> SELECT VARIABLE_NAME, VARIABLE_VALUE FROM performance_schema.global_variables where VARIABLE_NAME in ('password_history', 'password_reuse_interval');'`. I saw that the `password_history` and `password_reuse_interval` were set to `0` which means the password did not need to be different than past iterations.
![Pic2](/Report/artifacts/bac369-m29a.png)
2. Using the guidelines in the CIS benchmark, I added `SET PERSIST password_history = 5; SET PERSIST password_reuse_interval = 365;` to `setup.sql`. This updated the mysql file and changed password_history to 5 meaning the new password must be unique from the last 5 and password_reuse_interval meaning there a year before the password can be reused .
![Pic2](/Report/artifacts/bac369-m2729b.png)
3. I rebuilt the db and ran the command from 1) and saw the `password_history` was now set to 5 and `password_reuse_interval` was now set to 365. This control now passes.
![Pic2](/Report/artifacts/bac369-m2729b.png)

### Control 20: 4.2
1. I ran `SELECT * FROM information_schema.SCHEMATA where SCHEMA_NAME not in ('mysql','information_schema', 'sys', 'performance_schema');` to find the list of currents databases.
2. I disagree with previous FAIL assessment in the Security Review. The only database was the `GiftCardSiteDB`. This database does not appear a test database. It should remain. No further action is required for this control.
![Pic2](/Report/artifacts/bac369-42a.png)

## Part 2
##### Code:  [job-secrets.yaml](/jobs/k8/job-secrets.yaml) , [job-passcheck1.yaml](/jobs/k8/job-passcheck1.yaml) , [job-tablecheck.yaml](/jobs/k8/job-tablecheck.yaml)

I used Kubernetes Cronjobs to automate the checking of the password settings (Control 2.7) and the databases Control 4.2). Navigating Cronjobs and inter-cluster interactions was challenging at first, but I found ways to simplify things.
Since a new IP address was assigned to the MYSQL container every time `minikube` was started, I assigned a static IP address by adding a `cluster IP: 10.106.166.130` to `db-service.yaml`.
![Pic2](/Report/artifacts/bac369-p21.png)
To access the mysql server. I created the job pods using the `assign3-db:v0` image since it has MYSQL installed. It is scheduled for `0 * * * *`, which means it will run the job every hour on the hour at minute `00`. For the commands I used `/bin/sh -c` to issue commands and `mysql -h $(MYSQL_SERVICE_IP) -u root -p$(MYSQL_ROOT_PASSWORD)` to connect to the server. The IP is passed through an enviroment variable in the yaml, the password is passed through the `jobs-secrets.yaml` to maintain good practice.
Finally I used the queries listed in the benchmark documentation to query the dbs for the value.
All the job yaml files can be seen below:
![Pic2](/Report/artifacts/bac369-p22.png)
I scheduled the jobs by running `kubectl create -f jobs/k8`, and used `kubectl get jobs --watch` to see their status. Once they were completed I used the pod selector to print out the logs of the two jobs. As you can see below, the output of `kubectl logs $pod` for `passheck.yaml` the `password_history` and `password_reuse_interval` are printed in the logs and for `tablecheck.yaml` the dbs are printed in the logs. As scheduled the, job took place at the 00 minute in the two hours it has been running.
![Pic2](/Report/artifacts/bac369-p23.png)
